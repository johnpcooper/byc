{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alignment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import rank, threshold_otsu, sobel_v\n",
    "from skimage.morphology import disk, remove_small_objects\n",
    "from skimage import transform\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.feature import register_translation\n",
    "from skimage.transform import AffineTransform\n",
    "import skimage.io as io\n",
    "from skimage.util import img_as_uint\n",
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "import logging\n",
    "import tifffile as tf\n",
    "import tkinter as tk\n",
    "import tkinter.filedialog as dia\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants(object):\n",
    "    FIFTEEN_DEGREES_IN_RADIANS = 0.262\n",
    "    ACCEPTABLE_SKEW_THRESHOLD = 5.0\n",
    "    NUM_CATCH_CHANNELS = 28\n",
    "    \n",
    "def create_vertical_segments(image_data):\n",
    "    \"\"\"\n",
    "    Creates a binary image with blobs surrounding areas that have a lot of vertical edges\n",
    "    \"\"\"\n",
    "    # find edges that have a strong vertical direction\n",
    "    vertical_edges = sobel_v(image_data)\n",
    "    # Sepearate out the areas where there is a large amount of vertically\n",
    "    # oriented stuff\n",
    "    return _segment_edge_areas(vertical_edges)\n",
    "    \n",
    "def _segment_edge_areas(edges, disk_size=9, mean_threshold=200, min_object_size=500):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes a greyscale image (with brighter colors corresponding to edges) and returns\n",
    "    a binary image with high edge density and black indicates low density\n",
    "    \n",
    "    param image_data: a 2D numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the greyscale edge information into black and white image\n",
    "    threshold = threshold_otsu(edges)\n",
    "    # Filter out the edge data below the threshold, effectively removing some noise\n",
    "    raw_channel_areas = edges <= threshold\n",
    "    # smooth out the data\n",
    "    channel_areas = rank.mean(raw_channel_areas, disk(disk_size)) < mean_threshold\n",
    "    # remove specks and blobs that are the result of artifacts\n",
    "    clean_channel_areas = remove_small_objects(channel_areas, min_size=min_object_size)\n",
    "    # Fill in any areas that re completely surrounded by the areas (hopefully) covering\n",
    "    # the channels\n",
    "    return ndimage.binary_fill_holes(clean_channel_areas)\n",
    "\n",
    "\n",
    "# from fylm/service/rotation.py\n",
    "def _determine_rotation_offset(image):\n",
    "    \"\"\"\n",
    "    Finds rotational skew so that the sides of the central trench are (nearly) perfectly vertical.\n",
    "    :param image:   raw image data in a 2D (i.e. grayscale) numpy array\n",
    "    :type image:    np.array()\n",
    "    \"\"\"\n",
    "    segmentation = create_vertical_segments(image)\n",
    "    # Draw a line that follows the center of the segments at each point, which should be roughly vertical\n",
    "    # We should expect this to give us four approximately-vertical lines, possibly with many gaps in each line\n",
    "    skeletons = skeletonize(segmentation)\n",
    "    # Use the Hough transform to get the closest lines that approximate those four lines\n",
    "    hough = transform.hough_line(skeletons, np.arange(-Constants.FIFTEEN_DEGREES_IN_RADIANS,\n",
    "                                                      Constants.FIFTEEN_DEGREES_IN_RADIANS,\n",
    "                                                      0.0001))\n",
    "    # Create a list of the angles (in radians) of all of the lines the Hough transform produced, with 0.0 being\n",
    "    # completely vertical\n",
    "    # These angles correspond to the angles of the four sides of the channels, which we need to correct for\n",
    "    angles = [angle for _, angle, dist in zip(*transform.hough_line_peaks(*hough))]\n",
    "    if not angles:\n",
    "        log.warn(\"Image skew could not be calculated. The image is probably invalid.\")\n",
    "        return 0.0\n",
    "    else:\n",
    "        # Get the average angle and convert it to degrees\n",
    "        offset = sum(angles) / len(angles) * 180.0 / math.pi\n",
    "        if offset > Constants.ACCEPTABLE_SKEW_THRESHOLD:\n",
    "            log.warn(\"Image is heavily skewed. Check that the images are valid.\")\n",
    "        return offset\n",
    "    \n",
    "def rotate_image(image, offset):\n",
    "    \"\"\"\n",
    "    \n",
    "    Return an image (np.array()) rotated by the number of degrees\n",
    "    returned by _determine_rotation_offset(image)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return transform.rotate(image, offset)\n",
    "\n",
    "# from fylm/service/registration.py\n",
    "def _determine_registration_offset(base_image, uncorrected_image):\n",
    "    \"\"\"\n",
    "    \n",
    "    Finds the translational offset required to align this image with all others in the stack.\n",
    "    Returns dx, dy adjustments in pixels *but does not change the image!*\n",
    "    \n",
    "    :param base_image:   a 2D numpy array that the other image should be aligned to\n",
    "    :param uncorrected_image:   a 2D numpy array\n",
    "    :returns:   float, float\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get the dimensions of the images that we're aligning\n",
    "    base_height, base_width = base_image.shape\n",
    "    uncorrected_height, uncorrected_width = uncorrected_image.shape\n",
    "\n",
    "    # We take the area that roughly corresponds to the catch channels. This has two benefits: one, it\n",
    "    # speeds up the registration significantly (as it scales linearly with image size), and two, if\n",
    "    # a large amount of debris/yeast/bacteria/whatever shows up in the central trench, the registration\n",
    "    # algorithm goes bonkers if it's considering that portion of the image.\n",
    "    # Thus we separately find the registration for the left side and right side, and average them.\n",
    "    left_base_section = base_image[:, int(base_width * 0.1): int(base_width * 0.3)]\n",
    "    left_uncorrected = uncorrected_image[:, int(uncorrected_width * 0.1): int(uncorrected_width * 0.3)]\n",
    "    right_base_section = base_image[:, int(base_width * 0.7): int(base_width * 0.9)]\n",
    "    right_uncorrected = uncorrected_image[:, int(uncorrected_width * 0.7): int(uncorrected_width * 0.9)]\n",
    "\n",
    "    # \n",
    "    left_dy, left_dx = register_translation(left_base_section, left_uncorrected, upsample_factor=20)[0]\n",
    "    right_dy, right_dx = register_translation(right_base_section, right_uncorrected, upsample_factor=20)[0]\n",
    "\n",
    "    return (left_dy + right_dy) / 2.0, (left_dx + right_dx) / 2.0\n",
    "\n",
    "def translate_image(uncorrected_image, translational_offset):\n",
    "    x = translational_offset[1]\n",
    "    y = translational_offset[0]\n",
    "    new_image = transform.warp(uncorrected_image, transform.AffineTransform(translation=(-x, -y)))\n",
    "    return new_image\n",
    "\n",
    "def get_channel_names(fov_path):\n",
    "\n",
    "    fov_slices_filenames = os.listdir(fov_path)\n",
    "    rep_image_filename = fov_slices_filenames[0]\n",
    "    rep_image = tf.imread(fov_path + '/%s' % rep_image_filename)\n",
    "    len_channels = len(rep_image)\n",
    "    # if it's a single channel image, len_channels will return number of pixels, not \n",
    "    # the actual number of channels so I'm filtering for that here\n",
    "    if len_channels > 10:\n",
    "        len_channels = 1\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    print(\"Detected {} channels\".format(len_channels))\n",
    "    print(\"Enter names below:\")\n",
    "\n",
    "    channel_names = []\n",
    "    for i in range(0, len_channels):\n",
    "        channel_name = input(\"Name for channel {}: \".format(i))\n",
    "        channel_names.append(channel_name)\n",
    "\n",
    "    return len_channels, channel_names\n",
    "\n",
    "def align_images(fov_path, channel_names):\n",
    "\n",
    "    fov_slice_filenames = os.listdir(fov_path)\n",
    "\n",
    "    images = []\n",
    "    for filename in fov_slice_filenames:\n",
    "        image = tf.imread(fov_path + '/%s' % filename)\n",
    "        images.append(image)\n",
    "        print(filename)\n",
    "\n",
    "    # ascertain the number of channels in the image, assume that \n",
    "    # channel 0 is brightfield\n",
    "\n",
    "    # Calculate rotational offset for three random frames\n",
    "    # in FOV stack\n",
    "    if len(images) >= 3:\n",
    "        frames_to_align = np.random.choice(range(len(images)), 3, replace=False)\n",
    "    elif len(image) < 3:\n",
    "        frames_to_align = [0]\n",
    "\n",
    "    rotational_offsets = []\n",
    "    for frame in frames_to_align:\n",
    "        image = images[frame]    \n",
    "        print(f\"Determining rotation offset for frame {frame}\")\n",
    "        if image.ndim < 3: # if image only has one channel\n",
    "        # assume that that one channel is the brightfield channel\n",
    "        # and align based on that channel\n",
    "            vis_channel = image\n",
    "        elif image.ndim >= 3: # if image has more than one channel,\n",
    "        # align based on the first channel which should be brightfied (bf)\n",
    "            vis_channel = image[0]\n",
    "        rotational_offset = _determine_rotation_offset(vis_channel)\n",
    "        print(rotational_offset)\n",
    "        rotational_offsets.append(rotational_offset)\n",
    "\n",
    "    rotational_offset = np.min(np.array(rotational_offsets))\n",
    "    final_rt_offests_arr = np.full(shape=(len(images)), fill_value=(rotational_offset))\n",
    "\n",
    "    # create a list of rotationally aligned images rotated according to the final_rt_offsets_arr array\n",
    "    rotated_images = []\n",
    "    index = 0 # again, progress bar index\n",
    "    for i, frame in enumerate(images):\n",
    "        print(\"Rotating image %d of %d\" % (i, len(images)))\n",
    "\n",
    "        if frame.ndim < 3:\n",
    "            rotated_channels_i = rotate_image(frame, final_rt_offests_arr[i])\n",
    "        elif frame.ndim >= 3:\n",
    "            channels_i = []\n",
    "            for j in range(0, len(frame)):\n",
    "                channels_i.append(frame[j])\n",
    "\n",
    "            index += 1\n",
    "\n",
    "            rotated_channels_i = []\n",
    "            for j in range(0, len(channels_i)):\n",
    "                rotated_channels_i.append(rotate_image(channels_i[j], final_rt_offests_arr[i]))\n",
    "\n",
    "        rotated_images.append(rotated_channels_i)\n",
    "    \n",
    "    # calculate translational offsets\n",
    "    index = 0\n",
    "    translational_offsets = []\n",
    "    for i in range(0, len(rotated_images)):\n",
    "        image = rotated_images[i][0]\n",
    "        index += 1\n",
    "        print(\"Determining registration offset %d of %d\" % (index, len(images)))\n",
    "        try:\n",
    "            # align based on feautres of image 0, I think I should change this to align\n",
    "            # to image i-1\n",
    "            translational_offsets.append(_determine_registration_offset(rotated_images[0][0], image))\n",
    "        except:\n",
    "            translational_offsets.append(_determine_registration_offset(rotated_images[0], rotated_images[i]))\n",
    "        \n",
    "    # now translate the images\n",
    "    translated_images = []\n",
    "    index = 0\n",
    "    for i in range(0, len(translational_offsets)):\n",
    "        index += 1\n",
    "        print(\"Translating image %d of %d\" % (index, len(images)))\n",
    "        \n",
    "        translated_channels_i = []\n",
    "\n",
    "        try:\n",
    "            for j in range(0, len(images[0])):\n",
    "                translated_channels_i.append(translate_image(rotated_images[i][j], translational_offsets[i]))\n",
    "        except:\n",
    "            translated_channels_i = (translate_image(rotated_images[i], translational_offsets[i]))\n",
    "\n",
    "        translated_images.append(translated_channels_i)\n",
    "    \n",
    "    # make a dictionary to hold images for each channel\n",
    "    keys = channel_names\n",
    "    values = []\n",
    "\n",
    "    if len(channel_names) > 1:\n",
    "        for j in range(0, len(images[0])):        \n",
    "            values.append([translated_images[i][j] for i in range(0, len(translated_images))])\n",
    "    else:\n",
    "        values.append([translated_images[i] for i in range(0, len(translated_images))])\n",
    "\n",
    "    translated_images_dict = dict(zip(keys, values))\n",
    "\n",
    "    return translated_images_dict\n",
    "\n",
    "def save_stacks(translated_images_dict, save_path, fov_name):\n",
    "    print(\"Saving stacks...\")\n",
    "    for channel, stack in translated_images_dict.items():\n",
    "        print(\"Saving {} stack\".format(channel))\n",
    "        concat_stack = io.concatenate_images(img_as_uint(stack))\n",
    "        tf.imsave(save_path + '/{}_{}_stack.tif'.format(fov_name, channel), concat_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_alignment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byc import alignment as a\n",
    "import tifffile as tf\n",
    "import tkinter as tk\n",
    "import tkinter.filedialog as dia\n",
    "import os\n",
    "\n",
    "def set_paths():\n",
    "    \n",
    "    \"\"\" Return input_path and output_path, two directories chosen by the user. \n",
    "        Input_path should contain the sorted xy directories for each fov. \"\"\"\n",
    "    \n",
    "    # Choose the directory holding all the fields of view that you'll align\n",
    "    root = tk.Tk()\n",
    "    input_path = dia.askdirectory(parent=root,\n",
    "                                  title='Choose the directory holding the experiment you want to align')\n",
    "    root.destroy()\n",
    "\n",
    "    # ask the user where they would like to save the output stacks\n",
    "    root = tk.Tk()\n",
    "    output_path = dia.askdirectory(parent=root,\n",
    "                                   title='Choose the directory where you want to save aligned images')\n",
    "    root.destroy()\n",
    "    \n",
    "    return input_path, output_path\n",
    "\n",
    "# Generate a list of directories, one for each fov directory in the expt_path\n",
    "def set_fov_dirs(expt_path):\n",
    "\n",
    "    fov_dirs = os.listdir(expt_path)\n",
    "    fov_paths = []\n",
    "\n",
    "    for directory in fov_dirs:\n",
    "        fov_paths.append(expt_path + '/' + directory)\n",
    "    return fov_dirs, fov_paths\n",
    "\n",
    "def run():\n",
    "    expt_path, save_path = set_paths()\n",
    "    fov_dirs, fov_paths = set_fov_dirs(expt_path)\n",
    "    len_channels, channel_names = a.get_channel_names(fov_paths[0])\n",
    "\n",
    "    for i in range(0, len(fov_dirs)):\n",
    "\n",
    "        fov_name = fov_dirs[i]\n",
    "        fov_path = fov_paths[i]\n",
    "        print(\"Aligning %s\" % fov_name)\n",
    "\n",
    "        translated_images_dict = a.align_images(fov_path, channel_names)\n",
    "        a.save_stacks(translated_images_dict, save_path, fov_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/John Cooper/Box Sync/Finkelstein-Matouschek/byc_data/example_byc_expts/20200221_byc/tifs/alignment_input'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 3 channels\n",
      "Enter names below:\n",
      "Name for channel 0: bf\n",
      "Name for channel 1: dsred\n",
      "Name for channel 2: yfp\n"
     ]
    }
   ],
   "source": [
    "fov_dirs, fov_paths = set_fov_dirs(expt_path)\n",
    "len_channels, channel_names = a.get_channel_names(fov_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning 20200221_byc_xy00\n",
      "20200214_byc_xy00_t000.tif\n",
      "20200214_byc_xy00_t001.tif\n",
      "20200214_byc_xy00_t002.tif\n",
      "20200214_byc_xy00_t003.tif\n",
      "20200214_byc_xy00_t004.tif\n",
      "20200214_byc_xy00_t005.tif\n",
      "20200214_byc_xy00_t006.tif\n",
      "20200214_byc_xy00_t007.tif\n",
      "20200214_byc_xy00_t008.tif\n",
      "20200214_byc_xy00_t009.tif\n",
      "20200214_byc_xy00_t010.tif\n",
      "Determining rotation offset for frame 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dtype cannot be bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7b43d60c4094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Aligning %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfov_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtranslated_images_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfov_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_stacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslated_images_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfov_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\byc\\byc\\alignment.py\u001b[0m in \u001b[0;36malign_images\u001b[1;34m(fov_path, channel_names)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# align based on the first channel which should be brightfied (bf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mvis_channel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mrotational_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_determine_rotation_offset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis_channel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotational_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mrotational_offsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotational_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\byc\\byc\\alignment.py\u001b[0m in \u001b[0;36m_determine_rotation_offset\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0msegmentation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vertical_segments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;31m# Draw a line that follows the center of the segments at each point, which should be roughly vertical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# We should expect this to give us four approximately-vertical lines, possibly with many gaps in each line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\byc\\byc\\alignment.py\u001b[0m in \u001b[0;36mcreate_vertical_segments\u001b[1;34m(image_data)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Sepearate out the areas where there is a large amount of vertically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# oriented stuff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_segment_edge_areas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertical_edges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_segment_edge_areas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_object_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\byc\\byc\\alignment.py\u001b[0m in \u001b[0;36m_segment_edge_areas\u001b[1;34m(edges, disk_size, mean_threshold, min_object_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mraw_channel_areas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medges\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# smooth out the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mchannel_areas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_channel_areas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmean_threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# remove specks and blobs that are the result of artifacts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mclean_channel_areas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_small_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_areas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_object_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(image, selem, out, mask, shift_x, shift_y)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \"\"\"\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m     return _apply_scalar_per_pixel(generic_cy._mean, image, selem, out=out,\n\u001b[0m\u001b[0;32m    505\u001b[0m                                    mask=mask, shift_x=shift_x, shift_y=shift_y)\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\u001b[0m in \u001b[0;36m_apply_scalar_per_pixel\u001b[1;34m(func, image, selem, out, mask, shift_x, shift_y, out_dtype)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \"\"\"\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# preprocess and verify the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     image, selem, out, mask, n_bins = _preprocess_input(image, selem,\n\u001b[0m\u001b[0;32m    182\u001b[0m                                                         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                                                         out_dtype)\n",
      "\u001b[1;32mC:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\u001b[0m in \u001b[0;36m_preprocess_input\u001b[1;34m(image, selem, out, mask, out_dtype, pixel_size)\u001b[0m\n\u001b[0;32m    104\u001b[0m     if (input_dtype in (bool, np.bool, np.bool_)\n\u001b[0;32m    105\u001b[0m             or out_dtype in (bool, np.bool, np.bool_)):\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dtype cannot be bool.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_dtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         message = ('Possible precision loss converting image of type {} to '\n",
      "\u001b[1;31mValueError\u001b[0m: dtype cannot be bool."
     ]
    }
   ],
   "source": [
    "for i in range(0, len(fov_dirs)):\n",
    "\n",
    "    fov_name = fov_dirs[i]\n",
    "    fov_path = fov_paths[i]\n",
    "    print(\"Aligning %s\" % fov_name)\n",
    "\n",
    "    translated_images_dict = a.align_images(fov_path, channel_names)\n",
    "    a.save_stacks(translated_images_dict, save_path, fov_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    File \"C:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\", line 504, in mean\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    " File \"C:\\Users\\John Cooper\\Projects\\byc\\byc\\alignment.py\", line 56, in _segment_edge_areas\n",
    "    channel_areas = rank.mean(raw_channel_areas, disk(disk_size)) < mean_threshold\n",
    "  File \"C:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\", line 504, in mean\n",
    "    return _apply_scalar_per_pixel(generic_cy._mean, image, selem, out=out,\n",
    "  File \"C:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\", line 181, in _apply_scalar_per_pixel\n",
    "    image, selem, out, mask, n_bins = _preprocess_input(image, selem,\n",
    "  File \"C:\\.main_env\\lib\\site-packages\\skimage\\filters\\rank\\generic.py\", line 106, in _preprocess_input\n",
    "    raise ValueError('dtype cannot be bool.')\n",
    "ValueError: dtype cannot be bool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200214_byc_xy00_t000.tif',\n",
       " '20200214_byc_xy00_t001.tif',\n",
       " '20200214_byc_xy00_t002.tif',\n",
       " '20200214_byc_xy00_t003.tif',\n",
       " '20200214_byc_xy00_t004.tif',\n",
       " '20200214_byc_xy00_t005.tif',\n",
       " '20200214_byc_xy00_t006.tif',\n",
       " '20200214_byc_xy00_t007.tif',\n",
       " '20200214_byc_xy00_t008.tif',\n",
       " '20200214_byc_xy00_t009.tif',\n",
       " '20200214_byc_xy00_t010.tif']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(fov_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200214_byc_xy00_t000.tif\n",
      "20200214_byc_xy00_t001.tif\n",
      "20200214_byc_xy00_t002.tif\n",
      "20200214_byc_xy00_t003.tif\n",
      "20200214_byc_xy00_t004.tif\n",
      "20200214_byc_xy00_t005.tif\n",
      "20200214_byc_xy00_t006.tif\n",
      "20200214_byc_xy00_t007.tif\n",
      "20200214_byc_xy00_t008.tif\n",
      "20200214_byc_xy00_t009.tif\n",
      "20200214_byc_xy00_t010.tif\n",
      "Determining rotation offset for frame 0\n"
     ]
    }
   ],
   "source": [
    "fov_path = fov_paths[0]\n",
    "fov_slice_filenames = os.listdir(fov_path)\n",
    "frame = 0\n",
    "\n",
    "images = []\n",
    "for filename in fov_slice_filenames:\n",
    "    image = tf.imread(fov_path + '/%s' % filename)\n",
    "    images.append(image)\n",
    "    print(filename)\n",
    "image = images[frame]   \n",
    "print(f\"Determining rotation offset for frame {frame}\")\n",
    "if image.ndim < 3: # if image only has one channel\n",
    "# assume that that one channel is the brightfield channel\n",
    "# and align based on that channel\n",
    "    vis_channel = image\n",
    "elif image.ndim >= 3: # if image has more than one channel,\n",
    "# align based on the first channel which should be brightfied (bf)\n",
    "    vis_channel = image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vertical_segments(image_data):\n",
    "    \"\"\"\n",
    "    Creates a binary image with blobs surrounding areas that have a lot of vertical edges\n",
    "    \"\"\"\n",
    "    # find edges that have a strong vertical direction\n",
    "    vertical_edges = sobel_v(image_data)\n",
    "    # Sepearate out the areas where there is a large amount of vertically\n",
    "    # oriented stuff\n",
    "    return _segment_edge_areas(vertical_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edges = sobel_v(vis_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-60082e8b6d4e>:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  edges[[raw_channel_areas_bool]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.0004387 , -0.00018311,  0.00392157, ..., -0.00151446,\n",
       "       -0.00141527, -0.00035859])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_channel_areas - 1) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\.main_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3331: UserWarning: Possible precision loss converting image of type int32 to uint8 as required by rank filters. Convert manually using skimage.util.img_as_ubyte to silence this warning.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\.main_env\\lib\\site-packages\\skimage\\util\\dtype.py:525: UserWarning: Downcasting int32 to uint8 without scaling because max value 1 fits in uint8\n",
      "  return _convert(image, np.uint8, force_copy)\n"
     ]
    }
   ],
   "source": [
    "# _segment_vertical_edges()\n",
    "disk_size=9\n",
    "\n",
    "mean_threshold=200\n",
    "min_object_size=500\n",
    "\n",
    "edges = vertical_edges\n",
    "# convert the greyscale edge information into black and white image\n",
    "threshold = threshold_otsu(edges)\n",
    "# Filter out the edge data below the threshold, effectively removing some noise\n",
    "raw_channel_areas_bool = edges <= threshold\n",
    "raw_channel_areas = raw_channel_areas_bool.astype(np.int)\n",
    "raw_channel_areas = (raw_channel_areas-1)*-1\n",
    "# smooth out the data\n",
    "channel_areas = rank.mean(raw_channel_areas, disk(disk_size)) < mean_threshold\n",
    "# remove specks and blobs that are the result of artifacts\n",
    "clean_channel_areas = remove_small_objects(channel_areas, min_size=min_object_size)\n",
    "# Fill in any areas that re completely surrounded by the areas (hopefully) covering\n",
    "# the channels\n",
    "final = ndimage.binary_fill_holes(clean_channel_areas)\n",
    "\n",
    "segmentation = final\n",
    "# Draw a line that follows the center of the segments at each point, which should be roughly vertical\n",
    "# We should expect this to give us four approximately-vertical lines, possibly with many gaps in each line\n",
    "skeletons = skeletonize(raw_channel_areas)\n",
    "# Use the Hough transform to get the closest lines that approximate those four lines\n",
    "hough = transform.hough_line(skeletons, np.arange(-Constants.FIFTEEN_DEGREES_IN_RADIANS,\n",
    "                                                  Constants.FIFTEEN_DEGREES_IN_RADIANS,\n",
    "                                                  0.0001))\n",
    "# Create a list of the angles (in radians) of all of the lines the Hough transform produced, with 0.0 being\n",
    "# completely vertical\n",
    "# These angles correspond to the angles of the four sides of the channels, which we need to correct for\n",
    "angles = [angle for _, angle, dist in zip(*transform.hough_line_peaks(*hough))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x298a96aeb50>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skimage.io.imshow(skeletonize(raw_channel_areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.hough_line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023736822939477605"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = sum(angles) / len(angles) * 180.0 / math.pi\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndimage.binary_fill_holes(raw_channel_areas_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndimage.binary_fill_holes(raw_channel_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_channel_areas_bool[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29886d49340>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skimage.io.imshow(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_channel_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019866955932421618"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images(fov_path, channel_names):\n",
    "\n",
    "    fov_slice_filenames = os.listdir(fov_path)\n",
    "\n",
    "    images = []\n",
    "    for filename in fov_slice_filenames:\n",
    "        image = tf.imread(fov_path + '/%s' % filename)\n",
    "        images.append(image)\n",
    "        print(filename)\n",
    "\n",
    "    # ascertain the number of channels in the image, assume that \n",
    "    # channel 0 is brightfield\n",
    "\n",
    "    # Calculate rotational offset for three random frames\n",
    "    # in FOV stack\n",
    "    if len(images) >= 3:\n",
    "        frames_to_align = np.random.choice(range(len(images)), 3, replace=False)\n",
    "    elif len(image) < 3:\n",
    "        frames_to_align = [0]\n",
    "\n",
    "    rotational_offsets = []\n",
    "    for frame in frames_to_align:\n",
    "        image = images[frame]    \n",
    "        print(f\"Determining rotation offset for frame {frame}\")\n",
    "        if image.ndim < 3: # if image only has one channel\n",
    "        # assume that that one channel is the brightfield channel\n",
    "        # and align based on that channel\n",
    "            vis_channel = image\n",
    "        elif image.ndim >= 3: # if image has more than one channel,\n",
    "        # align based on the first channel which should be brightfied (bf)\n",
    "            vis_channel = image[0]\n",
    "        rotational_offset = _determine_rotation_offset(vis_channel)\n",
    "        print(rotational_offset)\n",
    "        rotational_offsets.append(rotational_offset)\n",
    "\n",
    "    rotational_offset = np.min(np.array(rotational_offsets))\n",
    "    final_rt_offests_arr = np.full(shape=(len(images)), fill_value=(rotational_offset))\n",
    "\n",
    "    # create a list of rotationally aligned images rotated according to the final_rt_offsets_arr array\n",
    "    rotated_images = []\n",
    "    index = 0 # again, progress bar index\n",
    "    for i, frame in enumerate(images):\n",
    "        print(\"Rotating image %d of %d\" % (i, len(images)))\n",
    "\n",
    "        if frame.ndim < 3:\n",
    "            rotated_channels_i = rotate_image(frame, final_rt_offests_arr[i])\n",
    "        elif frame.ndim >= 3:\n",
    "            channels_i = []\n",
    "            for j in range(0, len(frame)):\n",
    "                channels_i.append(frame[j])\n",
    "\n",
    "            index += 1\n",
    "\n",
    "            rotated_channels_i = []\n",
    "            for j in range(0, len(channels_i)):\n",
    "                rotated_channels_i.append(rotate_image(channels_i[j], final_rt_offests_arr[i]))\n",
    "\n",
    "        rotated_images.append(rotated_channels_i)\n",
    "    \n",
    "    # calculate translational offsets\n",
    "    index = 0\n",
    "    translational_offsets = []\n",
    "    for i in range(0, len(rotated_images)):\n",
    "        image = rotated_images[i][0]\n",
    "        index += 1\n",
    "        print(\"Determining registration offset %d of %d\" % (index, len(images)))\n",
    "        try:\n",
    "            # align based on feautres of image 0, I think I should change this to align\n",
    "            # to image i-1\n",
    "            translational_offsets.append(_determine_registration_offset(rotated_images[0][0], image))\n",
    "        except:\n",
    "            translational_offsets.append(_determine_registration_offset(rotated_images[0], rotated_images[i]))\n",
    "        \n",
    "    # now translate the images\n",
    "    translated_images = []\n",
    "    index = 0\n",
    "    for i in range(0, len(translational_offsets)):\n",
    "        index += 1\n",
    "        print(\"Translating image %d of %d\" % (index, len(images)))\n",
    "        \n",
    "        translated_channels_i = []\n",
    "\n",
    "        try:\n",
    "            for j in range(0, len(images[0])):\n",
    "                translated_channels_i.append(translate_image(rotated_images[i][j], translational_offsets[i]))\n",
    "        except:\n",
    "            translated_channels_i = (translate_image(rotated_images[i], translational_offsets[i]))\n",
    "\n",
    "        translated_images.append(translated_channels_i)\n",
    "    \n",
    "    # make a dictionary to hold images for each channel\n",
    "    keys = channel_names\n",
    "    values = []\n",
    "\n",
    "    if len(channel_names) > 1:\n",
    "        for j in range(0, len(images[0])):        \n",
    "            values.append([translated_images[i][j] for i in range(0, len(translated_images))])\n",
    "    else:\n",
    "        values.append([translated_images[i] for i in range(0, len(translated_images))])\n",
    "\n",
    "    translated_images_dict = dict(zip(keys, values))\n",
    "\n",
    "    return translated_images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants(object):\n",
    "    FIFTEEN_DEGREES_IN_RADIANS = 0.262\n",
    "    ACCEPTABLE_SKEW_THRESHOLD = 5.0\n",
    "    NUM_CATCH_CHANNELS = 28\n",
    "    \n",
    "def create_vertical_segments(image_data):\n",
    "    \"\"\"\n",
    "    Creates a binary image with blobs surrounding areas that have a lot of vertical edges\n",
    "    \"\"\"\n",
    "    # find edges that have a strong vertical direction\n",
    "    vertical_edges = sobel_v(image_data)\n",
    "    # Sepearate out the areas where there is a large amount of vertically\n",
    "    # oriented stuff\n",
    "    return _segment_edge_areas(vertical_edges)\n",
    "    \n",
    "def _segment_edge_areas(edges, disk_size=9, mean_threshold=200, min_object_size=500):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes a greyscale image (with brighter colors corresponding to edges) and returns\n",
    "    a binary image with high edge density and black indicates low density\n",
    "    \n",
    "    param image_data: a 2D numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the greyscale edge information into black and white image\n",
    "    threshold = threshold_otsu(edges)\n",
    "    # Filter out the edge data below the threshold, effectively removing some noise\n",
    "    raw_channel_areas = edges <= threshold\n",
    "    # smooth out the data\n",
    "    channel_areas = rank.mean(raw_channel_areas, disk(disk_size)) < mean_threshold\n",
    "    # remove specks and blobs that are the result of artifacts\n",
    "    clean_channel_areas = remove_small_objects(channel_areas, min_size=min_object_size)\n",
    "    # Fill in any areas that re completely surrounded by the areas (hopefully) covering\n",
    "    # the channels\n",
    "    return ndimage.binary_fill_holes(clean_channel_areas)\n",
    "# from fylm/service/rotation.py\n",
    "def _determine_rotation_offset(image):\n",
    "    \"\"\"\n",
    "    Finds rotational skew so that the sides of the central trench are (nearly) perfectly vertical.\n",
    "    :param image:   raw image data in a 2D (i.e. grayscale) numpy array\n",
    "    :type image:    np.array()\n",
    "    \"\"\"\n",
    "    segmentation = create_vertical_segments(image)\n",
    "    # Draw a line that follows the center of the segments at each point, which should be roughly vertical\n",
    "    # We should expect this to give us four approximately-vertical lines, possibly with many gaps in each line\n",
    "    skeletons = skeletonize(segmentation)\n",
    "    # Use the Hough transform to get the closest lines that approximate those four lines\n",
    "    hough = transform.hough_line(skeletons, np.arange(-Constants.FIFTEEN_DEGREES_IN_RADIANS,\n",
    "                                                      Constants.FIFTEEN_DEGREES_IN_RADIANS,\n",
    "                                                      0.0001))\n",
    "    # Create a list of the angles (in radians) of all of the lines the Hough transform produced, with 0.0 being\n",
    "    # completely vertical\n",
    "    # These angles correspond to the angles of the four sides of the channels, which we need to correct for\n",
    "    angles = [angle for _, angle, dist in zip(*transform.hough_line_peaks(*hough))]\n",
    "    if not angles:\n",
    "        log.warn(\"Image skew could not be calculated. The image is probably invalid.\")\n",
    "        return 0.0\n",
    "    else:\n",
    "        # Get the average angle and convert it to degrees\n",
    "        offset = sum(angles) / len(angles) * 180.0 / math.pi\n",
    "        if offset > Constants.ACCEPTABLE_SKEW_THRESHOLD:\n",
    "            log.warn(\"Image is heavily skewed. Check that the images are valid.\")\n",
    "        return offset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".main_env",
   "language": "python",
   "name": ".main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
